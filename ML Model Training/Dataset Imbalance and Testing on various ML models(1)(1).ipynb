{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b1b7bda",
   "metadata": {},
   "source": [
    "# Imbalance in the dataset\n",
    "\n",
    "In a dataset, class imbalance refers to a situation where the number of instances of one class greatly outnumbers the instances of another class (or classes). This can lead to biased models that perform poorly on the minority class.\n",
    "we can analyze the imbalance of a dataset from model analysis parameters like accuracy and confusion matrix.\n",
    "\n",
    "If your dataset is imbalanced, there are several methods to address this issue:\n",
    "\n",
    "## 1)Resampling:\n",
    "Either oversample the minority class (add more instances of the minority class) or undersample the majority class (remove instances of the majority class).\n",
    "\n",
    "## 2)Synthetic Minority Over-sampling Technique (SMOTE): \n",
    "Generate synthetic samples for the minority class to balance the dataset.\n",
    "\n",
    "## 3)Weighted Loss Function:\n",
    "Adjust the loss function to penalize misclassifications of the minority class more than the majority class.\n",
    "\n",
    "## 4)Different Algorithms: \n",
    "Some algorithms, like Random Forests, handle imbalanced datasets better than others. Consider using algorithms that are less sensitive to class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff8acd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is balanced.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_excel('train_data.xlsx')\n",
    "\n",
    "def check_imbalance(df, label_column, threshold=0.05):\n",
    "    label_counts = df[label_column].value_counts()\n",
    "    class_proportions = label_counts / label_counts.sum()\n",
    "    return any(class_proportions < threshold)\n",
    "    \n",
    "label = 'Sarcasm'    \n",
    "# Check imbalance\n",
    "is_imbalanced = check_imbalance(df, label)\n",
    "if is_imbalanced:\n",
    "    print(\"The dataset is imbalanced.\")\n",
    "else:\n",
    "    print(\"The dataset is balanced.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3786d70a",
   "metadata": {},
   "source": [
    "# Performance Metrics\n",
    "\n",
    "## 1) accuracy_score\n",
    "\n",
    "In scikit-learn, the accuracy_score function is used to calculate the accuracy of a classification model. Accuracy is defined as the ratio of correctly predicted instances to the total instances in the dataset. It is a simple and intuitive metric for evaluating classification models. \n",
    "\n",
    "## Accuracy= Number of Correct Predictions/Total Number of Predictions\n",
    "\n",
    "A high accuracy score (close to 1) generally indicates good model performance, meaning most predictions are correct.\n",
    "Accuracy is a reliable metric when the classes are balanced\n",
    "\n",
    "## 2) confusion_matrics\n",
    "\n",
    "A confusion matrix is a powerful tool for evaluating the performance of a classification model. It provides a more detailed breakdown of correct and incorrect classifications than overall accuracy.\n",
    "\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model.\n",
    "\n",
    "                                Predicted Positive\tPredicted Negative\n",
    "## Actual Positive\tTrue Positive (TP)\tFalse Negative (FN)\n",
    "## Actual Negative\tFalse Positive (FP)\tTrue Negative (TN)\n",
    "\n",
    "True Positives (TP): Correctly predicted positive instances.\n",
    "True Negatives (TN): Correctly predicted negative instances.\n",
    "False Positives (FP): Incorrectly predicted positive instances (Type I error).\n",
    "False Negatives (FN): Incorrectly predicted negative instances (Type II error).\n",
    "\n",
    "High TP and TN, Low FP and FN: A good model will have high true positive and true negative values and low false positive and false negative values.\n",
    "\n",
    "## 3) classification_report\n",
    "\n",
    "The classification_report function in sklearn provides a detailed summary of the performance of a classification algorithm. It includes key metrics such as precision, recall, f1-score, and support for each class. \n",
    "\n",
    "Precision: The ratio of correctly predicted positive observations to the total predicted positives. Precision is a measure of how accurate the model is in identifying relevant instances.\n",
    "\n",
    "## Precision = True Positives/(True Positives + False Positives)\n",
    "\n",
    "\n",
    "Recall (Sensitivity or True Positive Rate): The ratio of correctly predicted positive observations to all observations in the actual class. Recall measures how well the model captures all relevant instances.\n",
    "\n",
    "## Recall = True Positives/(True Positives + False Negatives)\n",
    " \n",
    " \n",
    "F1-Score: The weighted average of precision and recall. The F1-score is especially useful when you need to balance precision and recall.\n",
    "\n",
    "## F1-Score = 2 × (Precision+Recall/Precision×Recall)\n",
    "\n",
    "Support: The number of actual occurrences of the class in the dataset. This helps understand the class distribution in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a939b849",
   "metadata": {},
   "source": [
    "# ML models training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "778e2412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d63b2cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_excel('train_data.xlsx')\n",
    "\n",
    "X_train = train['idf_vector']\n",
    "y_train = train['Sarcasm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6b9614",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_excel('test_data.xlsx')\n",
    "\n",
    "X_test = test['idf_vector']\n",
    "y_test = test['Sarcasm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad2d9251",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train.apply(lambda x: np.fromstring(x[1:-1], sep=' ')).tolist())\n",
    "X_test = np.array(X_test.apply(lambda x: np.fromstring(x[1:-1], sep=' ')).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3990b249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_data</th>\n",
       "      <th>Sarcasm</th>\n",
       "      <th>cbow_vectors</th>\n",
       "      <th>skip_vectors</th>\n",
       "      <th>idf_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Ive', 'already', 'see', 'spinoffs', 'cartoon...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ 1.33140460e-02 -4.40596461e-01 -2.24290624e-...</td>\n",
       "      <td>[-0.08790181  0.13337336  0.20003186  0.057840...</td>\n",
       "      <td>[0.35064284 0.         0.70874616 0.35064284 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['probably', 'one', 'bad', 'movie', 'ever', 'm...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-7.44502991e-03 -4.45567846e-01 -2.15648413e-...</td>\n",
       "      <td>[-0.09143309  0.1359427   0.20129208  0.058086...</td>\n",
       "      <td>[0.36828071 0.         0.74407735 0.36828071 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Paint', 'number', 'story', 'mediocre', 'act'...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.03100936 -0.44244248 -0.228988   -0.594159...</td>\n",
       "      <td>[-0.08539815  0.13671964  0.1906442   0.051597...</td>\n",
       "      <td>[0.33881107 0.00515968 0.68165561 0.33881107 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['first', 'murder', 'scene', 'one', 'best', 'm...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.00549362 -0.3918013  -0.20509245 -0.564492...</td>\n",
       "      <td>[-0.0883963   0.1333404   0.1942192   0.059124...</td>\n",
       "      <td>[0.3352069  0.         0.68956848 0.3352069  0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['Bravo', 'another', 'movie', 'hero', 'deep', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.04189784 -0.3942514  -0.22329089 -0.573215...</td>\n",
       "      <td>[-0.08794053  0.14131702  0.19568387  0.059249...</td>\n",
       "      <td>[0.32067146 0.         0.7215108  0.32067146 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>['love', 'movie', 'manage', 'suck', 'joy', 'li...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.01969795 -0.46055502 -0.23612061 -0.589896...</td>\n",
       "      <td>[-9.14753377e-02  1.41389504e-01  1.97079882e-...</td>\n",
       "      <td>[0.33714827 0.         0.73559621 0.33714827 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5193</th>\n",
       "      <td>['Yet', 'another', 'adventure', 'movie', 'prot...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.0546301  -0.41608295 -0.22733516 -0.576408...</td>\n",
       "      <td>[-0.07994005  0.13224454  0.19289885  0.049859...</td>\n",
       "      <td>[0.30734299 0.         0.68298441 0.30734299 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5194</th>\n",
       "      <td>['Yet', 'another', 'forgettable', 'action', 'f...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.03428297 -0.43000963 -0.23234665 -0.575691...</td>\n",
       "      <td>[-8.8605739e-02  1.3400672e-01  1.9444640e-01 ...</td>\n",
       "      <td>[0.29527348 0.         0.68897146 0.29527348 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5195</th>\n",
       "      <td>['Id', 'rather', 'stick', 'elevator', 'mime', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.0534298  -0.39407605 -0.23116787 -0.555099...</td>\n",
       "      <td>[-9.07627866e-02  1.49938092e-01  1.94540560e-...</td>\n",
       "      <td>[0.31039716 0.         0.74495319 0.31039716 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>['Boris', 'Karloff', 'Bela', 'Lugosi', 'make',...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ 0.0134683  -0.42515242 -0.22746369 -0.612809...</td>\n",
       "      <td>[-0.08633483  0.13501793  0.19534539  0.056610...</td>\n",
       "      <td>[0.35299828 0.         0.70760842 0.35299828 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5197 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             train_data  Sarcasm  \\\n",
       "0     ['Ive', 'already', 'see', 'spinoffs', 'cartoon...        0   \n",
       "1     ['probably', 'one', 'bad', 'movie', 'ever', 'm...        1   \n",
       "2     ['Paint', 'number', 'story', 'mediocre', 'act'...        0   \n",
       "3     ['first', 'murder', 'scene', 'one', 'best', 'm...        0   \n",
       "4     ['Bravo', 'another', 'movie', 'hero', 'deep', ...        1   \n",
       "...                                                 ...      ...   \n",
       "5192  ['love', 'movie', 'manage', 'suck', 'joy', 'li...        1   \n",
       "5193  ['Yet', 'another', 'adventure', 'movie', 'prot...        1   \n",
       "5194  ['Yet', 'another', 'forgettable', 'action', 'f...        1   \n",
       "5195  ['Id', 'rather', 'stick', 'elevator', 'mime', ...        1   \n",
       "5196  ['Boris', 'Karloff', 'Bela', 'Lugosi', 'make',...        0   \n",
       "\n",
       "                                           cbow_vectors  \\\n",
       "0     [ 1.33140460e-02 -4.40596461e-01 -2.24290624e-...   \n",
       "1     [-7.44502991e-03 -4.45567846e-01 -2.15648413e-...   \n",
       "2     [-0.03100936 -0.44244248 -0.228988   -0.594159...   \n",
       "3     [-0.00549362 -0.3918013  -0.20509245 -0.564492...   \n",
       "4     [-0.04189784 -0.3942514  -0.22329089 -0.573215...   \n",
       "...                                                 ...   \n",
       "5192  [-0.01969795 -0.46055502 -0.23612061 -0.589896...   \n",
       "5193  [-0.0546301  -0.41608295 -0.22733516 -0.576408...   \n",
       "5194  [-0.03428297 -0.43000963 -0.23234665 -0.575691...   \n",
       "5195  [-0.0534298  -0.39407605 -0.23116787 -0.555099...   \n",
       "5196  [ 0.0134683  -0.42515242 -0.22746369 -0.612809...   \n",
       "\n",
       "                                           skip_vectors  \\\n",
       "0     [-0.08790181  0.13337336  0.20003186  0.057840...   \n",
       "1     [-0.09143309  0.1359427   0.20129208  0.058086...   \n",
       "2     [-0.08539815  0.13671964  0.1906442   0.051597...   \n",
       "3     [-0.0883963   0.1333404   0.1942192   0.059124...   \n",
       "4     [-0.08794053  0.14131702  0.19568387  0.059249...   \n",
       "...                                                 ...   \n",
       "5192  [-9.14753377e-02  1.41389504e-01  1.97079882e-...   \n",
       "5193  [-0.07994005  0.13224454  0.19289885  0.049859...   \n",
       "5194  [-8.8605739e-02  1.3400672e-01  1.9444640e-01 ...   \n",
       "5195  [-9.07627866e-02  1.49938092e-01  1.94540560e-...   \n",
       "5196  [-0.08633483  0.13501793  0.19534539  0.056610...   \n",
       "\n",
       "                                             idf_vector  \n",
       "0     [0.35064284 0.         0.70874616 0.35064284 0...  \n",
       "1     [0.36828071 0.         0.74407735 0.36828071 0...  \n",
       "2     [0.33881107 0.00515968 0.68165561 0.33881107 0...  \n",
       "3     [0.3352069  0.         0.68956848 0.3352069  0...  \n",
       "4     [0.32067146 0.         0.7215108  0.32067146 0...  \n",
       "...                                                 ...  \n",
       "5192  [0.33714827 0.         0.73559621 0.33714827 0...  \n",
       "5193  [0.30734299 0.         0.68298441 0.30734299 0...  \n",
       "5194  [0.29527348 0.         0.68897146 0.29527348 0...  \n",
       "5195  [0.31039716 0.         0.74495319 0.31039716 0...  \n",
       "5196  [0.35299828 0.         0.70760842 0.35299828 0...  \n",
       "\n",
       "[5197 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41521d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_input(model, input_vector):\n",
    "    prediction = model.predict([input_vector])\n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b4cbc4",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Logistic regression is a statistical model used for binary classification tasks, where the goal is to predict the probability that an instance belongs to a particular class.\n",
    "\n",
    "The logistic regression model in `sklearn` begins by initializing the weights and intercept to small random values or zeros. It then calculates a weighted sum of the input features and applies the sigmoid function to convert this sum into probabilities. To measure the difference between the predicted probabilities and the actual labels, the logistic loss (binary cross-entropy) is computed. The model uses gradient descent to iteratively update the weights by calculating the gradient of the cost function and minimizing it. Finally, the model converts the predicted probabilities to binary outcomes using a threshold, typically set at 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3aeab64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the given vectors:\n",
      "\n",
      "Accuracy: 0.75\n",
      "Confusion Matrix:\n",
      "[[488 116]\n",
      " [204 492]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.81      0.75       604\n",
      "           1       0.81      0.71      0.75       696\n",
      "\n",
      "    accuracy                           0.75      1300\n",
      "   macro avg       0.76      0.76      0.75      1300\n",
      "weighted avg       0.76      0.75      0.75      1300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "def logistic_regression(X_train, X_test, y_train, y_test):\n",
    "    lr_model = LogisticRegression(max_iter=1000)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    lr_preds = lr_model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, lr_preds)\n",
    "    conf_matrix = confusion_matrix(y_test, lr_preds)\n",
    "    class_report = classification_report(y_test,lr_preds)\n",
    "\n",
    "    print(f\"Evaluation for the given vectors:\\n\")\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "    print('Classification Report:')\n",
    "    print(class_report)\n",
    "    \n",
    "    return lr_model\n",
    "\n",
    "reg_model=logistic_regression(X_train,X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1bbb91",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Naive Bayes is a probabilistic classifier based on Bayes' theorem with strong (naive) independence assumptions between the features. It assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. Even though these assumptions are often violated in real-world data, Naive Bayes classifiers have been found to perform well in practice, especially for text classification tasks. They are simple, fast, and require a small amount of training data.\n",
    "\n",
    "Naive Bayes in `sklearn` starts by assuming that the features are conditionally independent given the class label. The model calculates the prior probabilities of each class based on the training data. For each feature, it estimates the likelihood of the feature value given each class using the appropriate probability distribution (e.g., Gaussian for continuous data, multinomial for discrete data). During prediction, it combines these likelihoods with the prior probabilities using Bayes' theorem to compute the posterior probabilities for each class. The model then assigns the class label with the highest posterior probability to the input data. Naive Bayes in `sklearn` uses efficient methods to handle these computations, making it suitable for both small and large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab242cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the given vectors:\n",
      "\n",
      "Accuracy: 0.55\n",
      "Confusion Matrix:\n",
      "[[ 27 577]\n",
      " [ 13 683]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.04      0.08       604\n",
      "           1       0.54      0.98      0.70       696\n",
      "\n",
      "    accuracy                           0.55      1300\n",
      "   macro avg       0.61      0.51      0.39      1300\n",
      "weighted avg       0.60      0.55      0.41      1300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "def naive_bayes(X_train, X_test, y_train, y_test):\n",
    "    nb_model = MultinomialNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    nb_preds = nb_model.predict(X_test\n",
    "                               )\n",
    "    accuracy = accuracy_score(y_test, nb_preds)\n",
    "    conf_matrix = confusion_matrix(y_test, nb_preds)\n",
    "    class_report = classification_report(y_test,nb_preds)\n",
    "\n",
    "    print(f\"Evaluation for the given vectors:\\n\")\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "    print('Classification Report:')\n",
    "    print(class_report)\n",
    "    return nb_model\n",
    "\n",
    "bayes_model = naive_bayes( X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105ec807",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "A decision tree is a flowchart-like tree structure where an internal node represents a \"test\" on an attribute (e.g., whether a word appears in a document), a branch represents the outcome of the test, and each leaf node represents a class label (e.g., sarcastic or not sarcastic). The paths from the root to the leaf represent classification rules.\n",
    "\n",
    "Decision trees are easy to interpret and visualize, making them useful for understanding the data. However, they can be prone to overfitting, especially with complex trees and noisy data.\n",
    "\n",
    "Decision trees in `sklearn` start by selecting the best feature to split the data based on a criterion such as Gini impurity or information gain. The model recursively partitions the dataset into subsets that contain instances with similar values for the selected feature, creating nodes and branches. This process continues until all leaves are pure (contain only instances of a single class) or a stopping criterion is met (such as maximum depth or minimum samples per leaf). At each node, the model chooses the split that results in the most significant reduction in impurity. During prediction, the model traverses the tree from the root to a leaf node, following the branches based on the feature values of the input data, and assigns the class label of the reached leaf node. `sklearn` implements decision trees efficiently, allowing for fast training and prediction on both small and large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14792900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the given vectors:\n",
      "\n",
      "Accuracy: 0.71\n",
      "Confusion Matrix:\n",
      "[[426 178]\n",
      " [198 498]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.71      0.69       604\n",
      "           1       0.74      0.72      0.73       696\n",
      "\n",
      "    accuracy                           0.71      1300\n",
      "   macro avg       0.71      0.71      0.71      1300\n",
      "weighted avg       0.71      0.71      0.71      1300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Decision Tree\n",
    "def decision_tree(X_train, X_test, y_train, y_test):\n",
    "    dt_model = DecisionTreeClassifier()\n",
    "    dt_model.fit(X_train, y_train)\n",
    "    dt_preds = dt_model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, dt_preds)\n",
    "    conf_matrix = confusion_matrix(y_test, dt_preds)\n",
    "    class_report = classification_report(y_test, dt_preds)\n",
    "\n",
    "    print(f\"Evaluation for the given vectors:\\n\")\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "    print('Classification Report:')\n",
    "    print(class_report)\n",
    "\n",
    "    return dt_model\n",
    "\n",
    "model_dec=decision_tree(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd5e5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b181ca7",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "Support Vector Machine is a powerful supervised machine learning algorithm that can be used for classification tasks. SVM finds the hyperplane that best separates the different classes in the feature space. It maximizes the margin, which is the distance between the hyperplane and the nearest data point from either class. SVM can handle high-dimensional data and works well in cases where the number of dimensions is greater than the number of samples.\n",
    "\n",
    "Support Vector Machines (SVM) in `sklearn` aim to find the optimal hyperplane that separates the data points of different classes with the maximum margin. The model identifies support vectors, which are the data points closest to the hyperplane, and these vectors influence the position and orientation of the hyperplane. SVM uses a cost function to balance maximizing the margin and minimizing classification errors. For non-linearly separable data, SVM employs kernel functions (such as linear, polynomial, or radial basis function) to transform the input space into a higher-dimensional space where a linear separation is possible.For prediction, the model classifies new data points based on which side of the hyperplane they fall on. `sklearn` uses efficient algorithms to perform these computations, making SVM suitable for both small and large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aabd2e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the given vectors:\n",
      "\n",
      "Accuracy: 0.77\n",
      "Confusion Matrix:\n",
      "[[504 100]\n",
      " [204 492]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77       604\n",
      "           1       0.83      0.71      0.76       696\n",
      "\n",
      "    accuracy                           0.77      1300\n",
      "   macro avg       0.77      0.77      0.77      1300\n",
      "weighted avg       0.78      0.77      0.77      1300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def svm(X_train, X_test, y_train, y_test):\n",
    "    svm_model = SVC()\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    svm_preds = svm_model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, svm_preds)\n",
    "    conf_matrix = confusion_matrix(y_test, svm_preds)\n",
    "    class_report = classification_report(y_test, svm_preds)\n",
    "\n",
    "    print(f\"Evaluation for the given vectors:\\n\")\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "    print('Classification Report:')\n",
    "    print(class_report)\n",
    "\n",
    "    return svm_model\n",
    "\n",
    "# Assuming X_train, y_train, X_test, and y_test are already defined\n",
    "model_svm = svm(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33315029",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Random Forest is an ensemble learning method that operates by constructing a multitude of decision trees at training time and outputs the class that is the mode of the classes of the individual trees. It combines the predictions of multiple decision trees to improve generalization and robustness over a single tree. Random forests are less likely to overfit than a single decision tree and are effective for both classification and regression tasks.\n",
    "\n",
    "Each of these methods has its own strengths and weaknesses, and the choice of which one to use depends on the specific characteristics of your dataset and the nature of the problem you are trying to solve.\n",
    "\n",
    "Random Forest in `sklearn` is an ensemble learning method that builds multiple decision trees during training and merges their results to improve accuracy and control overfitting. Each tree in the forest is trained on a different subset of the training data, created using bootstrapping (random sampling with replacement). Additionally, at each split in a tree, a random subset of features is selected to determine the best split, adding further randomness and reducing correlation between the trees. During prediction, each tree in the forest provides a classification, and the Random Forest model outputs the class that receives the majority vote from all the trees. This aggregation of results from multiple trees helps in reducing variance and improving the overall model performance. `sklearn` implements Random Forest efficiently, making it suitable for handling large datasets and high-dimensional data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21892d1e-b5c3-446a-b226-43208e949524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the given vectors:\n",
      "\n",
      "Accuracy: 0.80\n",
      "Confusion Matrix:\n",
      "[[509  95]\n",
      " [167 529]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.84      0.80       604\n",
      "           1       0.85      0.76      0.80       696\n",
      "\n",
      "    accuracy                           0.80      1300\n",
      "   macro avg       0.80      0.80      0.80      1300\n",
      "weighted avg       0.80      0.80      0.80      1300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "def random_forest(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(f\"Evaluation for the given vectors:\\n\")\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "    print('Classification Report:')\n",
    "    print(class_report)\n",
    "\n",
    "    return model\n",
    "\n",
    "model_rf=random_forest(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e20c188",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "After analyzing the performance metrics of the Random Forest model, it is evident that the model performs well in classifying the dataset into its respective classes. \n",
    "\n",
    "The model achieved a high accuracy score of 0.80, indicating that it correctly predicted 80% of the instances. \n",
    "\n",
    "The confusion matrix further illustrates the model's effectiveness, with most predictions concentrated along the diagonal, indicating accurate classification.The diagonal of the confusion matrix represents correct predictions (true positives and true negatives), while off-diagonal elements indicate errors (false positives and false negatives). Therefore, a strong concentration along the diagonal indicates high accuracy and effective classification by the model.\n",
    "\n",
    "The classification report shows balanced precision and recall across classes, with class 1 achieving the highest F1-score of 0.87, indicating robust performance in identifying positive instances.\n",
    "\n",
    "Overall, the Random Forest model demonstrates strong predictive capability and generalization to new data, making it a suitable choice for this classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbb5c16-8cba-4271-bee9-29fc83ea7a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ec92a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
