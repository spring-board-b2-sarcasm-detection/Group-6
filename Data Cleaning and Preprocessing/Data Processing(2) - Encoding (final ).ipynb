{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ff05f38",
   "metadata": {},
   "source": [
    "# Data Processing - Encoding\n",
    "\n",
    "Word embeddings are a way to represent words as dense vectors, capturing semantic relationships between words based on their usage in context.  With word embeddings, users can analyze and process text data, perform sentiment analysis, text classification, language translation, and more. Users can easily integrate word embeddings into their NLP workflows, visualizing the relationships between words and exploring the semantic meaning of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038f39b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n",
    "#!pip uninstall nltk\n",
    "#!pip install nltk\n",
    "#!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1534747-9408-43b0-acd3-1b572176b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6015956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56a4b614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_data</th>\n",
       "      <th>word_token</th>\n",
       "      <th>sent_token</th>\n",
       "      <th>subword_token</th>\n",
       "      <th>char_data</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One reviewer mention watch 1 Oz episode youll ...</td>\n",
       "      <td>['One', 'reviewer', 'mention', 'watch', '1', '...</td>\n",
       "      <td>['One reviewer mention watch 1 Oz episode youl...</td>\n",
       "      <td>['One', 'Ġreviewer', 'Ġmention', 'Ġwatch', 'Ġ1...</td>\n",
       "      <td>['O', 'n', 'e', ' ', 'r', 'e', 'v', 'i', 'e', ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>not sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production. filming technique...</td>\n",
       "      <td>['wonderful', 'little', 'production', '.', 'fi...</td>\n",
       "      <td>['wonderful little production.', 'filming tech...</td>\n",
       "      <td>['w', 'onder', 'ful', 'Ġlittle', 'Ġproduction'...</td>\n",
       "      <td>['w', 'o', 'n', 'd', 'e', 'r', 'f', 'u', 'l', ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>not sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movie groundbreaking experience! Ive never see...</td>\n",
       "      <td>['movie', 'groundbreaking', 'experience', '!',...</td>\n",
       "      <td>['movie groundbreaking experience!', 'Ive neve...</td>\n",
       "      <td>['movie', 'Ġground', 'breaking', 'Ġexperience'...</td>\n",
       "      <td>['m', 'o', 'v', 'i', 'e', ' ', 'g', 'r', 'o', ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>think wonderful way spend time hot summer week...</td>\n",
       "      <td>['think', 'wonderful', 'way', 'spend', 'time',...</td>\n",
       "      <td>['think wonderful way spend time hot summer we...</td>\n",
       "      <td>['think', 'Ġwonderful', 'Ġway', 'Ġspend', 'Ġti...</td>\n",
       "      <td>['t', 'h', 'i', 'n', 'k', ' ', 'w', 'o', 'n', ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>not sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Basically there family little boy Jake think t...</td>\n",
       "      <td>['Basically', 'there', 'family', 'little', 'bo...</td>\n",
       "      <td>['Basically there family little boy Jake think...</td>\n",
       "      <td>['B', 'as', 'ically', 'Ġthere', 'Ġfamily', 'Ġl...</td>\n",
       "      <td>['B', 'a', 's', 'i', 'c', 'a', 'l', 'l', 'y', ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>movie idea character development muscle less b...</td>\n",
       "      <td>['movie', 'idea', 'character', 'development', ...</td>\n",
       "      <td>['movie idea character development muscle less...</td>\n",
       "      <td>['movie', 'Ġidea', 'Ġcharacter', 'Ġdevelopment...</td>\n",
       "      <td>['m', 'o', 'v', 'i', 'e', ' ', 'i', 'd', 'e', ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>guess run budget decent script.</td>\n",
       "      <td>['guess', 'run', 'budget', 'decent', 'script',...</td>\n",
       "      <td>['guess run budget decent script.']</td>\n",
       "      <td>['gu', 'ess', 'Ġrun', 'Ġbudget', 'Ġdecent', 'Ġ...</td>\n",
       "      <td>['g', 'u', 'e', 's', 's', ' ', 'r', 'u', 'n', ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>need plot explosion every five minutes?</td>\n",
       "      <td>['need', 'plot', 'explosion', 'every', 'five',...</td>\n",
       "      <td>['need plot explosion every five minutes?']</td>\n",
       "      <td>['need', 'Ġplot', 'Ġexplosion', 'Ġevery', 'Ġfi...</td>\n",
       "      <td>['n', 'e', 'e', 'd', ' ', 'p', 'l', 'o', 't', ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>award generic action movie ever made?</td>\n",
       "      <td>['award', 'generic', 'action', 'movie', 'ever'...</td>\n",
       "      <td>['award generic action movie ever made?']</td>\n",
       "      <td>['aw', 'ard', 'Ġgeneric', 'Ġaction', 'Ġmovie',...</td>\n",
       "      <td>['a', 'w', 'a', 'r', 'd', ' ', 'g', 'e', 'n', ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>Two hour nonstop mindnumbing action.</td>\n",
       "      <td>['Two', 'hour', 'nonstop', 'mindnumbing', 'act...</td>\n",
       "      <td>['Two hour nonstop mindnumbing action.']</td>\n",
       "      <td>['T', 'w', 'o', 'Ġhour', 'Ġnon', 'stop', 'Ġmin...</td>\n",
       "      <td>['T', 'w', 'o', ' ', 'h', 'o', 'u', 'r', ' ', ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sarcastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           cleaned_data  \\\n",
       "0     One reviewer mention watch 1 Oz episode youll ...   \n",
       "1     wonderful little production. filming technique...   \n",
       "2     movie groundbreaking experience! Ive never see...   \n",
       "3     think wonderful way spend time hot summer week...   \n",
       "4     Basically there family little boy Jake think t...   \n",
       "...                                                 ...   \n",
       "6492  movie idea character development muscle less b...   \n",
       "6493                    guess run budget decent script.   \n",
       "6494            need plot explosion every five minutes?   \n",
       "6495              award generic action movie ever made?   \n",
       "6496               Two hour nonstop mindnumbing action.   \n",
       "\n",
       "                                             word_token  \\\n",
       "0     ['One', 'reviewer', 'mention', 'watch', '1', '...   \n",
       "1     ['wonderful', 'little', 'production', '.', 'fi...   \n",
       "2     ['movie', 'groundbreaking', 'experience', '!',...   \n",
       "3     ['think', 'wonderful', 'way', 'spend', 'time',...   \n",
       "4     ['Basically', 'there', 'family', 'little', 'bo...   \n",
       "...                                                 ...   \n",
       "6492  ['movie', 'idea', 'character', 'development', ...   \n",
       "6493  ['guess', 'run', 'budget', 'decent', 'script',...   \n",
       "6494  ['need', 'plot', 'explosion', 'every', 'five',...   \n",
       "6495  ['award', 'generic', 'action', 'movie', 'ever'...   \n",
       "6496  ['Two', 'hour', 'nonstop', 'mindnumbing', 'act...   \n",
       "\n",
       "                                             sent_token  \\\n",
       "0     ['One reviewer mention watch 1 Oz episode youl...   \n",
       "1     ['wonderful little production.', 'filming tech...   \n",
       "2     ['movie groundbreaking experience!', 'Ive neve...   \n",
       "3     ['think wonderful way spend time hot summer we...   \n",
       "4     ['Basically there family little boy Jake think...   \n",
       "...                                                 ...   \n",
       "6492  ['movie idea character development muscle less...   \n",
       "6493                ['guess run budget decent script.']   \n",
       "6494        ['need plot explosion every five minutes?']   \n",
       "6495          ['award generic action movie ever made?']   \n",
       "6496           ['Two hour nonstop mindnumbing action.']   \n",
       "\n",
       "                                          subword_token  \\\n",
       "0     ['One', 'Ġreviewer', 'Ġmention', 'Ġwatch', 'Ġ1...   \n",
       "1     ['w', 'onder', 'ful', 'Ġlittle', 'Ġproduction'...   \n",
       "2     ['movie', 'Ġground', 'breaking', 'Ġexperience'...   \n",
       "3     ['think', 'Ġwonderful', 'Ġway', 'Ġspend', 'Ġti...   \n",
       "4     ['B', 'as', 'ically', 'Ġthere', 'Ġfamily', 'Ġl...   \n",
       "...                                                 ...   \n",
       "6492  ['movie', 'Ġidea', 'Ġcharacter', 'Ġdevelopment...   \n",
       "6493  ['gu', 'ess', 'Ġrun', 'Ġbudget', 'Ġdecent', 'Ġ...   \n",
       "6494  ['need', 'Ġplot', 'Ġexplosion', 'Ġevery', 'Ġfi...   \n",
       "6495  ['aw', 'ard', 'Ġgeneric', 'Ġaction', 'Ġmovie',...   \n",
       "6496  ['T', 'w', 'o', 'Ġhour', 'Ġnon', 'stop', 'Ġmin...   \n",
       "\n",
       "                                              char_data Sentiment  \\\n",
       "0     ['O', 'n', 'e', ' ', 'r', 'e', 'v', 'i', 'e', ...  positive   \n",
       "1     ['w', 'o', 'n', 'd', 'e', 'r', 'f', 'u', 'l', ...  positive   \n",
       "2     ['m', 'o', 'v', 'i', 'e', ' ', 'g', 'r', 'o', ...  positive   \n",
       "3     ['t', 'h', 'i', 'n', 'k', ' ', 'w', 'o', 'n', ...  positive   \n",
       "4     ['B', 'a', 's', 'i', 'c', 'a', 'l', 'l', 'y', ...  negative   \n",
       "...                                                 ...       ...   \n",
       "6492  ['m', 'o', 'v', 'i', 'e', ' ', 'i', 'd', 'e', ...  negative   \n",
       "6493  ['g', 'u', 'e', 's', 's', ' ', 'r', 'u', 'n', ...  negative   \n",
       "6494  ['n', 'e', 'e', 'd', ' ', 'p', 'l', 'o', 't', ...  negative   \n",
       "6495  ['a', 'w', 'a', 'r', 'd', ' ', 'g', 'e', 'n', ...  negative   \n",
       "6496  ['T', 'w', 'o', ' ', 'h', 'o', 'u', 'r', ' ', ...  negative   \n",
       "\n",
       "            Sarcasm  \n",
       "0     not sarcastic  \n",
       "1     not sarcastic  \n",
       "2         sarcastic  \n",
       "3     not sarcastic  \n",
       "4         sarcastic  \n",
       "...             ...  \n",
       "6492      sarcastic  \n",
       "6493      sarcastic  \n",
       "6494      sarcastic  \n",
       "6495      sarcastic  \n",
       "6496      sarcastic  \n",
       "\n",
       "[6497 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset excel sheet\n",
    "df = pd.read_excel('tokenized Dataset.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2729f479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "negative    4184\n",
       "positive    2300\n",
       "neutral       13\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c1757e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sarcasm\n",
       "sarcastic        3518\n",
       "not sarcastic    2979\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sarcasm'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50da68b4-21b1-4856-b6df-e31976cee2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train,Y_train,X_test,Y_test):\n",
    "    # Train and evaluate the model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, Y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(Y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(Y_test, y_pred)\n",
    "    class_report = classification_report(Y_test, y_pred)\n",
    "\n",
    "    print(f\"Evaluation for the given vectors:\\n\")\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "    print('Classification Report:')\n",
    "    print(class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b991c33",
   "metadata": {},
   "source": [
    "# 1) Label Encoder\n",
    "\n",
    "Label Encoder: Label encoding assigns a unique integer to each word in the vocabulary. While this method provides a compact representation, it does not capture semantic relationships between words.\n",
    "\n",
    "## Use: \n",
    "Assigning a unique integer to each word in the vocabulary, useful for basic text encoding.\n",
    "\n",
    "# Pros: \n",
    "Compact representation, easy to use.\n",
    "\n",
    "# Cons: \n",
    "Does not capture semantic relationships, may not be suitable for advanced NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79a2aa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of sentiment labels to numerical values:\n",
      "negative: 0\n",
      "neutral: 1\n",
      "positive: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       2\n",
       "1       2\n",
       "2       2\n",
       "3       2\n",
       "4       0\n",
       "       ..\n",
       "6492    0\n",
       "6493    0\n",
       "6494    0\n",
       "6495    0\n",
       "6496    0\n",
       "Name: Sentiment, Length: 6497, dtype: int32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "df['Sentiment'] = label_encoder.fit_transform(df['Sentiment'])\n",
    "\n",
    "mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "print(\"Mapping of sentiment labels to numerical values:\")\n",
    "for sentiment, label in mapping.items():\n",
    "    print(f\"{sentiment}: {label}\")\n",
    "\n",
    "df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2bff888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of sarcasm labels to numerical values:\n",
      "not sarcastic: 0\n",
      "sarcastic: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       1\n",
       "3       0\n",
       "4       1\n",
       "       ..\n",
       "6492    1\n",
       "6493    1\n",
       "6494    1\n",
       "6495    1\n",
       "6496    1\n",
       "Name: Sarcasm, Length: 6497, dtype: int32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "df['Sarcasm'] = label_encoder.fit_transform(df['Sarcasm'])\n",
    "\n",
    "mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "print(\"Mapping of sarcasm labels to numerical values:\")\n",
    "for sarcasm, label in mapping.items():\n",
    "    print(f\"{sarcasm}: {label}\")\n",
    "\n",
    "df['Sarcasm']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6d46c9",
   "metadata": {},
   "source": [
    "# 2) One Hot encoder\n",
    "\n",
    "One Hot Encoder: This technique represents each word as a binary vector where all elements are zero except for the index corresponding to the word's position in the vocabulary, which is set to one. This method is simple but results in high-dimensional sparse vectors.\n",
    "\n",
    "## Use: \n",
    "Representing words as binary vectors, useful for simple text classification tasks.\n",
    "\n",
    "## Pros: \n",
    "Simple to implement, preserves the exact word information.\n",
    "\n",
    "## Cons: \n",
    "Results in high-dimensional sparse vectors, does not capture semantic relationships between words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97449b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of Sarcasm labels to one-hot encoded columns:\n",
      "0: [1. 0.]\n",
      "1: [1. 0.]\n",
      "\n",
      "Unique Sarcasm labels:\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_df = pd.DataFrame(encoder.fit_transform(df[['Sarcasm']]), columns=encoder.get_feature_names_out(['Sarcasm']))\n",
    "\n",
    "print(\"Mapping of Sarcasm labels to one-hot encoded columns:\")\n",
    "for label, encoded_columns in zip(df['Sarcasm'].unique(), encoded_df.values):\n",
    "    print(f\"{label}: {encoded_columns}\")\n",
    "    \n",
    "print(\"\\nUnique Sarcasm labels:\")\n",
    "print(df['Sarcasm'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd78c713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_excel('test_data.xlsx')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65291204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_excel('train_data.xlsx')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c853554",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['word_token']\n",
    "Y=df['Sarcasm']\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "train['train_data']=X_train\n",
    "train['Sarcasm']=y_train\n",
    "\n",
    "test['test_data']=X_test\n",
    "test['Sarcasm']=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2d00db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_data</th>\n",
       "      <th>Sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>['Ive', 'already', 'see', 'spinoffs', 'cartoon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>['probably', 'one', 'bad', 'movie', 'ever', 'm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>['Paint', 'number', 'story', 'mediocre', 'act'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2927</th>\n",
       "      <td>['first', 'murder', 'scene', 'one', 'best', 'm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6063</th>\n",
       "      <td>['Bravo', 'another', 'movie', 'hero', 'deep', ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>['love', 'movie', 'manage', 'suck', 'joy', 'li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>['Yet', 'another', 'adventure', 'movie', 'prot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>['Yet', 'another', 'forgettable', 'action', 'f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>['Id', 'rather', 'stick', 'elevator', 'mime', ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>['Boris', 'Karloff', 'Bela', 'Lugosi', 'make',...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5197 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             train_data  Sarcasm\n",
       "1916  ['Ive', 'already', 'see', 'spinoffs', 'cartoon...        0\n",
       "947   ['probably', 'one', 'bad', 'movie', 'ever', 'm...        1\n",
       "877   ['Paint', 'number', 'story', 'mediocre', 'act'...        0\n",
       "2927  ['first', 'murder', 'scene', 'one', 'best', 'm...        0\n",
       "6063  ['Bravo', 'another', 'movie', 'hero', 'deep', ...        1\n",
       "...                                                 ...      ...\n",
       "3772  ['love', 'movie', 'manage', 'suck', 'joy', 'li...        1\n",
       "5191  ['Yet', 'another', 'adventure', 'movie', 'prot...        1\n",
       "5226  ['Yet', 'another', 'forgettable', 'action', 'f...        1\n",
       "5390  ['Id', 'rather', 'stick', 'elevator', 'mime', ...        1\n",
       "860   ['Boris', 'Karloff', 'Bela', 'Lugosi', 'make',...        0\n",
       "\n",
       "[5197 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f060bbcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_data</th>\n",
       "      <th>Sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3103</th>\n",
       "      <td>['First', 'Im', 'firefighter', 'Im', 'kind', '...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>['Stargate', 'SG1', 'follow', 'intergalactic',...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4761</th>\n",
       "      <td>['Thank', 'Hollywood', 'yet', 'another', 'come...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4690</th>\n",
       "      <td>['Wow', 'another', 'comedy', 'movie', 'recycle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4032</th>\n",
       "      <td>['mesmerize', 'exploration', 'human', 'conditi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>['fully', 'aware', 'statistical', 'data', 'rea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>['think', 'id', 'check', 'film', 'Im', 'curren...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4917</th>\n",
       "      <td>['Bravo', '!', 'Another', 'comedy', 'leave', '...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198</th>\n",
       "      <td>['watch', 'privileged', 'people', 'travel', 'e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5643</th>\n",
       "      <td>['Another', 'movie', 'timetravel', 'paradoxes'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1300 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              test_data  Sarcasm\n",
       "3103  ['First', 'Im', 'firefighter', 'Im', 'kind', '...        0\n",
       "1419  ['Stargate', 'SG1', 'follow', 'intergalactic',...        0\n",
       "4761  ['Thank', 'Hollywood', 'yet', 'another', 'come...        1\n",
       "4690  ['Wow', 'another', 'comedy', 'movie', 'recycle...        1\n",
       "4032  ['mesmerize', 'exploration', 'human', 'conditi...        0\n",
       "...                                                 ...      ...\n",
       "889   ['fully', 'aware', 'statistical', 'data', 'rea...        0\n",
       "2850  ['think', 'id', 'check', 'film', 'Im', 'curren...        0\n",
       "4917  ['Bravo', '!', 'Another', 'comedy', 'leave', '...        1\n",
       "5198  ['watch', 'privileged', 'people', 'travel', 'e...        1\n",
       "5643  ['Another', 'movie', 'timetravel', 'paradoxes'...        1\n",
       "\n",
       "[1300 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f62ef80",
   "metadata": {},
   "source": [
    "## 3) Word2Vec\n",
    "Word2Vec: Word2Vec is a popular word embedding technique that learns continuous representations of words based on their context in a large corpus of text. It captures semantic relationships between words and is able to represent words with similar meanings as vectors that are close to each other in the embedding space.\n",
    "\n",
    "# Use: \n",
    "Learning continuous word representations based on context, useful for capturing semantic relationships between words.\n",
    "\n",
    "## Pros: \n",
    "Captures complex word relationships, provides dense and relatively low-dimensional embeddings.\n",
    "\n",
    "## Cons: \n",
    "Requires a large amount of training data, may not work well for rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08b130a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "\n",
    "# Create CBOW model\n",
    "word_cbow = gensim.models.Word2Vec(train['train_data'], min_count=1, vector_size=100, window=5)\n",
    "\n",
    "# Create Skip Gram model\n",
    "word_skip = gensim.models.Word2Vec(train['train_data'], min_count=1, vector_size=100, window=5, sg=1)\n",
    "\n",
    "def average_word_vectors(word_vectors, vector_size=100):\n",
    "    valid_vectors = [vec for vec in word_vectors if not np.isnan(vec).any()]\n",
    "    if not valid_vectors:\n",
    "        return np.zeros(vector_size) \n",
    "    else:\n",
    "        return np.mean(valid_vectors, axis=0)\n",
    "\n",
    "\n",
    "train['cbow_vectors'] = train['train_data'].apply(lambda x: average_word_vectors([word_cbow.wv[word] for word in x if word in word_cbow.wv]))\n",
    "test['cbow_vectors'] = test['test_data'].apply(lambda x: average_word_vectors([word_cbow.wv[word] for word in x if word in word_cbow.wv]))\n",
    "\n",
    "train['skip_vectors'] = train['train_data'].apply(lambda x: average_word_vectors([word_skip.wv[word] for word in x if word in word_skip.wv]))\n",
    "test['skip_vectors'] =test['test_data'].apply(lambda x: average_word_vectors([word_skip.wv[word] for word in x if word in word_skip.wv]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d04915fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1916    [0.013314046, -0.44059646, -0.22429062, -0.594...\n",
       "947     [-0.00744503, -0.44556785, -0.21564841, -0.613...\n",
       "877     [-0.031009361, -0.44244248, -0.228988, -0.5941...\n",
       "2927    [-0.0054936158, -0.3918013, -0.20509245, -0.56...\n",
       "6063    [-0.041897837, -0.3942514, -0.22329089, -0.573...\n",
       "                              ...                        \n",
       "3772    [-0.019697953, -0.46055502, -0.23612061, -0.58...\n",
       "5191    [-0.054630097, -0.41608295, -0.22733516, -0.57...\n",
       "5226    [-0.034282967, -0.43000963, -0.23234665, -0.57...\n",
       "5390    [-0.053429805, -0.39407605, -0.23116787, -0.55...\n",
       "860     [0.013468297, -0.42515242, -0.22746369, -0.612...\n",
       "Name: cbow_vectors, Length: 5197, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['cbow_vectors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e8980b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the given vectors:\n",
      "\n",
      "Accuracy: 0.77\n",
      "Confusion Matrix:\n",
      "[[472 132]\n",
      " [161 535]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.78      0.76       604\n",
      "           1       0.80      0.77      0.79       696\n",
      "\n",
      "    accuracy                           0.77      1300\n",
      "   macro avg       0.77      0.78      0.77      1300\n",
      "weighted avg       0.78      0.77      0.77      1300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_forest(list(train['cbow_vectors']),train['Sarcasm'],list(test['cbow_vectors']),test['Sarcasm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8569a08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1916    [-0.08790181, 0.13337336, 0.20003186, 0.057840...\n",
       "947     [-0.091433086, 0.1359427, 0.20129208, 0.058086...\n",
       "877     [-0.085398145, 0.13671964, 0.1906442, 0.051597...\n",
       "2927    [-0.0883963, 0.1333404, 0.1942192, 0.05912442,...\n",
       "6063    [-0.08794053, 0.14131702, 0.19568387, 0.059249...\n",
       "                              ...                        \n",
       "3772    [-0.09147534, 0.1413895, 0.19707988, 0.0596708...\n",
       "5191    [-0.07994005, 0.13224454, 0.19289885, 0.049859...\n",
       "5226    [-0.08860574, 0.13400672, 0.1944464, 0.0613590...\n",
       "5390    [-0.09076279, 0.14993809, 0.19454056, 0.072606...\n",
       "860     [-0.08633483, 0.13501793, 0.19534539, 0.056610...\n",
       "Name: skip_vectors, Length: 5197, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['skip_vectors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4d5dba6-d804-483d-a87c-e24e56263191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the given vectors:\n",
      "\n",
      "Accuracy: 0.79\n",
      "Confusion Matrix:\n",
      "[[498 106]\n",
      " [164 532]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.82      0.79       604\n",
      "           1       0.83      0.76      0.80       696\n",
      "\n",
      "    accuracy                           0.79      1300\n",
      "   macro avg       0.79      0.79      0.79      1300\n",
      "weighted avg       0.80      0.79      0.79      1300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_forest(list(train['skip_vectors']), train['Sarcasm'],list(test['skip_vectors']),test['Sarcasm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc634eb",
   "metadata": {},
   "source": [
    "# 4) TF-IDF\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency): TF-IDF represents each word based on its frequency in a document relative to its frequency in the entire corpus. It helps to identify the importance of words in a document.\n",
    "\n",
    "## Use: \n",
    "Representing words based on their importance in a document relative to a corpus, useful for information retrieval and text mining tasks.\n",
    "\n",
    "## Pros: \n",
    "Captures word importance, handles common words well.\n",
    "\n",
    "## Cons: \n",
    "Does not capture word semantics directly, may require tuning for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30bbc6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def dummy(tokens):\n",
    "    return tokens\n",
    "\n",
    "def vectorizer_idf(reviews, vectorizer=None):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=dummy, preprocessor=dummy)\n",
    "    X = vectorizer.fit_transform(reviews)\n",
    "    return X, vectorizer\n",
    "\n",
    "vectorized_idf, vectorizer = vectorizer_idf(train['train_data'])\n",
    "train['idf_vector'] = list(vectorized_idf.toarray())\n",
    "\n",
    "vectorized_test_idf = vectorizer.transform(test['test_data'])\n",
    "test['idf_vector'] = list(vectorized_test_idf.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56c69180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1916    [0.35064283887050296, 0.0, 0.7087461636744208,...\n",
       "947     [0.368280708933376, 0.0, 0.744077350702127, 0....\n",
       "877     [0.3388110731482772, 0.005159675648729211, 0.6...\n",
       "2927    [0.3352069005162684, 0.0, 0.6895684810620378, ...\n",
       "6063    [0.3206714645962336, 0.0, 0.7215107953415255, ...\n",
       "                              ...                        \n",
       "3772    [0.3371482651873016, 0.0, 0.7355962149541126, ...\n",
       "5191    [0.30734298674525307, 0.0, 0.6829844149894513,...\n",
       "5226    [0.29527348491322486, 0.0, 0.6889714647975247,...\n",
       "5390    [0.31039716335284884, 0.0, 0.7449531920468372,...\n",
       "860     [0.3529982794775053, 0.0, 0.7076084232448622, ...\n",
       "Name: idf_vector, Length: 5197, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['idf_vector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e214c298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for the given vectors:\n",
      "\n",
      "Accuracy: 0.80\n",
      "Confusion Matrix:\n",
      "[[509  95]\n",
      " [167 529]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.84      0.80       604\n",
      "           1       0.85      0.76      0.80       696\n",
      "\n",
      "    accuracy                           0.80      1300\n",
      "   macro avg       0.80      0.80      0.80      1300\n",
      "weighted avg       0.80      0.80      0.80      1300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_forest(list(train['idf_vector']),train['Sarcasm'],list(test['idf_vector']),test['Sarcasm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d06bec6-082a-4eac-82ba-b7480a664a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a25d59af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_data</th>\n",
       "      <th>Sarcasm</th>\n",
       "      <th>cbow_vectors</th>\n",
       "      <th>skip_vectors</th>\n",
       "      <th>idf_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>['Ive', 'already', 'see', 'spinoffs', 'cartoon...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.013314046, -0.44059646, -0.22429062, -0.594...</td>\n",
       "      <td>[-0.08790181, 0.13337336, 0.20003186, 0.057840...</td>\n",
       "      <td>[0.35064283887050296, 0.0, 0.7087461636744208,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>['probably', 'one', 'bad', 'movie', 'ever', 'm...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.00744503, -0.44556785, -0.21564841, -0.613...</td>\n",
       "      <td>[-0.091433086, 0.1359427, 0.20129208, 0.058086...</td>\n",
       "      <td>[0.368280708933376, 0.0, 0.744077350702127, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>['Paint', 'number', 'story', 'mediocre', 'act'...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.031009361, -0.44244248, -0.228988, -0.5941...</td>\n",
       "      <td>[-0.085398145, 0.13671964, 0.1906442, 0.051597...</td>\n",
       "      <td>[0.3388110731482772, 0.005159675648729211, 0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2927</th>\n",
       "      <td>['first', 'murder', 'scene', 'one', 'best', 'm...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.0054936158, -0.3918013, -0.20509245, -0.56...</td>\n",
       "      <td>[-0.0883963, 0.1333404, 0.1942192, 0.05912442,...</td>\n",
       "      <td>[0.3352069005162684, 0.0, 0.6895684810620378, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6063</th>\n",
       "      <td>['Bravo', 'another', 'movie', 'hero', 'deep', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.041897837, -0.3942514, -0.22329089, -0.573...</td>\n",
       "      <td>[-0.08794053, 0.14131702, 0.19568387, 0.059249...</td>\n",
       "      <td>[0.3206714645962336, 0.0, 0.7215107953415255, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>['love', 'movie', 'manage', 'suck', 'joy', 'li...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.019697953, -0.46055502, -0.23612061, -0.58...</td>\n",
       "      <td>[-0.09147534, 0.1413895, 0.19707988, 0.0596708...</td>\n",
       "      <td>[0.3371482651873016, 0.0, 0.7355962149541126, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>['Yet', 'another', 'adventure', 'movie', 'prot...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.054630097, -0.41608295, -0.22733516, -0.57...</td>\n",
       "      <td>[-0.07994005, 0.13224454, 0.19289885, 0.049859...</td>\n",
       "      <td>[0.30734298674525307, 0.0, 0.6829844149894513,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>['Yet', 'another', 'forgettable', 'action', 'f...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.034282967, -0.43000963, -0.23234665, -0.57...</td>\n",
       "      <td>[-0.08860574, 0.13400672, 0.1944464, 0.0613590...</td>\n",
       "      <td>[0.29527348491322486, 0.0, 0.6889714647975247,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>['Id', 'rather', 'stick', 'elevator', 'mime', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.053429805, -0.39407605, -0.23116787, -0.55...</td>\n",
       "      <td>[-0.09076279, 0.14993809, 0.19454056, 0.072606...</td>\n",
       "      <td>[0.31039716335284884, 0.0, 0.7449531920468372,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>['Boris', 'Karloff', 'Bela', 'Lugosi', 'make',...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.013468297, -0.42515242, -0.22746369, -0.612...</td>\n",
       "      <td>[-0.08633483, 0.13501793, 0.19534539, 0.056610...</td>\n",
       "      <td>[0.3529982794775053, 0.0, 0.7076084232448622, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5197 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             train_data  Sarcasm  \\\n",
       "1916  ['Ive', 'already', 'see', 'spinoffs', 'cartoon...        0   \n",
       "947   ['probably', 'one', 'bad', 'movie', 'ever', 'm...        1   \n",
       "877   ['Paint', 'number', 'story', 'mediocre', 'act'...        0   \n",
       "2927  ['first', 'murder', 'scene', 'one', 'best', 'm...        0   \n",
       "6063  ['Bravo', 'another', 'movie', 'hero', 'deep', ...        1   \n",
       "...                                                 ...      ...   \n",
       "3772  ['love', 'movie', 'manage', 'suck', 'joy', 'li...        1   \n",
       "5191  ['Yet', 'another', 'adventure', 'movie', 'prot...        1   \n",
       "5226  ['Yet', 'another', 'forgettable', 'action', 'f...        1   \n",
       "5390  ['Id', 'rather', 'stick', 'elevator', 'mime', ...        1   \n",
       "860   ['Boris', 'Karloff', 'Bela', 'Lugosi', 'make',...        0   \n",
       "\n",
       "                                           cbow_vectors  \\\n",
       "1916  [0.013314046, -0.44059646, -0.22429062, -0.594...   \n",
       "947   [-0.00744503, -0.44556785, -0.21564841, -0.613...   \n",
       "877   [-0.031009361, -0.44244248, -0.228988, -0.5941...   \n",
       "2927  [-0.0054936158, -0.3918013, -0.20509245, -0.56...   \n",
       "6063  [-0.041897837, -0.3942514, -0.22329089, -0.573...   \n",
       "...                                                 ...   \n",
       "3772  [-0.019697953, -0.46055502, -0.23612061, -0.58...   \n",
       "5191  [-0.054630097, -0.41608295, -0.22733516, -0.57...   \n",
       "5226  [-0.034282967, -0.43000963, -0.23234665, -0.57...   \n",
       "5390  [-0.053429805, -0.39407605, -0.23116787, -0.55...   \n",
       "860   [0.013468297, -0.42515242, -0.22746369, -0.612...   \n",
       "\n",
       "                                           skip_vectors  \\\n",
       "1916  [-0.08790181, 0.13337336, 0.20003186, 0.057840...   \n",
       "947   [-0.091433086, 0.1359427, 0.20129208, 0.058086...   \n",
       "877   [-0.085398145, 0.13671964, 0.1906442, 0.051597...   \n",
       "2927  [-0.0883963, 0.1333404, 0.1942192, 0.05912442,...   \n",
       "6063  [-0.08794053, 0.14131702, 0.19568387, 0.059249...   \n",
       "...                                                 ...   \n",
       "3772  [-0.09147534, 0.1413895, 0.19707988, 0.0596708...   \n",
       "5191  [-0.07994005, 0.13224454, 0.19289885, 0.049859...   \n",
       "5226  [-0.08860574, 0.13400672, 0.1944464, 0.0613590...   \n",
       "5390  [-0.09076279, 0.14993809, 0.19454056, 0.072606...   \n",
       "860   [-0.08633483, 0.13501793, 0.19534539, 0.056610...   \n",
       "\n",
       "                                             idf_vector  \n",
       "1916  [0.35064283887050296, 0.0, 0.7087461636744208,...  \n",
       "947   [0.368280708933376, 0.0, 0.744077350702127, 0....  \n",
       "877   [0.3388110731482772, 0.005159675648729211, 0.6...  \n",
       "2927  [0.3352069005162684, 0.0, 0.6895684810620378, ...  \n",
       "6063  [0.3206714645962336, 0.0, 0.7215107953415255, ...  \n",
       "...                                                 ...  \n",
       "3772  [0.3371482651873016, 0.0, 0.7355962149541126, ...  \n",
       "5191  [0.30734298674525307, 0.0, 0.6829844149894513,...  \n",
       "5226  [0.29527348491322486, 0.0, 0.6889714647975247,...  \n",
       "5390  [0.31039716335284884, 0.0, 0.7449531920468372,...  \n",
       "860   [0.3529982794775053, 0.0, 0.7076084232448622, ...  \n",
       "\n",
       "[5197 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['train_data','Sarcasm',\"cbow_vectors\",\"skip_vectors\",\"idf_vector\"]].to_excel('train_data.xlsx', index=False)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2e7cc03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_data</th>\n",
       "      <th>Sarcasm</th>\n",
       "      <th>cbow_vectors</th>\n",
       "      <th>skip_vectors</th>\n",
       "      <th>idf_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3103</th>\n",
       "      <td>['First', 'Im', 'firefighter', 'Im', 'kind', '...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.010372605, -0.4567238, -0.22420675, -0.601...</td>\n",
       "      <td>[-0.08752405, 0.13397478, 0.1981876, 0.0568917...</td>\n",
       "      <td>[0.36049630600235416, 0.0, 0.7245442997485739,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>['Stargate', 'SG1', 'follow', 'intergalactic',...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.01640415, -0.42824283, -0.22872612, -0.591...</td>\n",
       "      <td>[-0.086026564, 0.13662885, 0.19442646, 0.05281...</td>\n",
       "      <td>[0.3469430496512779, 0.0, 0.6961100932105769, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4761</th>\n",
       "      <td>['Thank', 'Hollywood', 'yet', 'another', 'come...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.013093409, -0.40319425, -0.19710416, -0.573...</td>\n",
       "      <td>[-0.083819784, 0.13904773, 0.19411542, 0.06791...</td>\n",
       "      <td>[0.32716079103618195, 0.0, 0.7046540114625458,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4690</th>\n",
       "      <td>['Wow', 'another', 'comedy', 'movie', 'recycle...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.013250855, -0.41309604, -0.22917916, -0.58...</td>\n",
       "      <td>[-0.09016309, 0.13978377, 0.19731839, 0.054263...</td>\n",
       "      <td>[0.32695772882490065, 0.0, 0.7006237046247871,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4032</th>\n",
       "      <td>['mesmerize', 'exploration', 'human', 'conditi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.060444143, -0.40848058, -0.24205366, -0.56...</td>\n",
       "      <td>[-0.0845619, 0.14551754, 0.18676989, 0.0542966...</td>\n",
       "      <td>[0.2973630975888576, 0.0, 0.6541988146954868, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>['fully', 'aware', 'statistical', 'data', 'rea...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.0037335956, -0.4461988, -0.22805376, -0.60...</td>\n",
       "      <td>[-0.08555071, 0.13806508, 0.19561169, 0.056104...</td>\n",
       "      <td>[0.35472575920999566, 0.0, 0.712496374722223, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>['think', 'id', 'check', 'film', 'Im', 'curren...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.023508983, -0.45118797, -0.23361883, -0.59...</td>\n",
       "      <td>[-0.0873381, 0.13907409, 0.19539632, 0.0538376...</td>\n",
       "      <td>[0.3557137497276089, 0.0, 0.7146909283518013, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4917</th>\n",
       "      <td>['Bravo', '!', 'Another', 'comedy', 'leave', '...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.0036038794, -0.41219482, -0.21283714, -0.5...</td>\n",
       "      <td>[-0.09664625, 0.14576766, 0.19806981, 0.053484...</td>\n",
       "      <td>[0.32585506917624835, 0.08336784378677009, 0.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198</th>\n",
       "      <td>['watch', 'privileged', 'people', 'travel', 'e...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.058767185, -0.4177433, -0.19625926, -0.573...</td>\n",
       "      <td>[-0.091859296, 0.14635305, 0.19235623, 0.05341...</td>\n",
       "      <td>[0.32020592598212944, 0.0, 0.6986311112337369,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5643</th>\n",
       "      <td>['Another', 'movie', 'timetravel', 'paradoxes'...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.025248924, -0.36244118, -0.21850352, -0.49...</td>\n",
       "      <td>[-0.084929675, 0.15160246, 0.19343479, 0.05581...</td>\n",
       "      <td>[0.29275678353878465, 0.0, 0.6830991615904976,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1300 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              test_data  Sarcasm  \\\n",
       "3103  ['First', 'Im', 'firefighter', 'Im', 'kind', '...        0   \n",
       "1419  ['Stargate', 'SG1', 'follow', 'intergalactic',...        0   \n",
       "4761  ['Thank', 'Hollywood', 'yet', 'another', 'come...        1   \n",
       "4690  ['Wow', 'another', 'comedy', 'movie', 'recycle...        1   \n",
       "4032  ['mesmerize', 'exploration', 'human', 'conditi...        0   \n",
       "...                                                 ...      ...   \n",
       "889   ['fully', 'aware', 'statistical', 'data', 'rea...        0   \n",
       "2850  ['think', 'id', 'check', 'film', 'Im', 'curren...        0   \n",
       "4917  ['Bravo', '!', 'Another', 'comedy', 'leave', '...        1   \n",
       "5198  ['watch', 'privileged', 'people', 'travel', 'e...        1   \n",
       "5643  ['Another', 'movie', 'timetravel', 'paradoxes'...        1   \n",
       "\n",
       "                                           cbow_vectors  \\\n",
       "3103  [-0.010372605, -0.4567238, -0.22420675, -0.601...   \n",
       "1419  [-0.01640415, -0.42824283, -0.22872612, -0.591...   \n",
       "4761  [0.013093409, -0.40319425, -0.19710416, -0.573...   \n",
       "4690  [-0.013250855, -0.41309604, -0.22917916, -0.58...   \n",
       "4032  [-0.060444143, -0.40848058, -0.24205366, -0.56...   \n",
       "...                                                 ...   \n",
       "889   [-0.0037335956, -0.4461988, -0.22805376, -0.60...   \n",
       "2850  [-0.023508983, -0.45118797, -0.23361883, -0.59...   \n",
       "4917  [-0.0036038794, -0.41219482, -0.21283714, -0.5...   \n",
       "5198  [-0.058767185, -0.4177433, -0.19625926, -0.573...   \n",
       "5643  [-0.025248924, -0.36244118, -0.21850352, -0.49...   \n",
       "\n",
       "                                           skip_vectors  \\\n",
       "3103  [-0.08752405, 0.13397478, 0.1981876, 0.0568917...   \n",
       "1419  [-0.086026564, 0.13662885, 0.19442646, 0.05281...   \n",
       "4761  [-0.083819784, 0.13904773, 0.19411542, 0.06791...   \n",
       "4690  [-0.09016309, 0.13978377, 0.19731839, 0.054263...   \n",
       "4032  [-0.0845619, 0.14551754, 0.18676989, 0.0542966...   \n",
       "...                                                 ...   \n",
       "889   [-0.08555071, 0.13806508, 0.19561169, 0.056104...   \n",
       "2850  [-0.0873381, 0.13907409, 0.19539632, 0.0538376...   \n",
       "4917  [-0.09664625, 0.14576766, 0.19806981, 0.053484...   \n",
       "5198  [-0.091859296, 0.14635305, 0.19235623, 0.05341...   \n",
       "5643  [-0.084929675, 0.15160246, 0.19343479, 0.05581...   \n",
       "\n",
       "                                             idf_vector  \n",
       "3103  [0.36049630600235416, 0.0, 0.7245442997485739,...  \n",
       "1419  [0.3469430496512779, 0.0, 0.6961100932105769, ...  \n",
       "4761  [0.32716079103618195, 0.0, 0.7046540114625458,...  \n",
       "4690  [0.32695772882490065, 0.0, 0.7006237046247871,...  \n",
       "4032  [0.2973630975888576, 0.0, 0.6541988146954868, ...  \n",
       "...                                                 ...  \n",
       "889   [0.35472575920999566, 0.0, 0.712496374722223, ...  \n",
       "2850  [0.3557137497276089, 0.0, 0.7146909283518013, ...  \n",
       "4917  [0.32585506917624835, 0.08336784378677009, 0.7...  \n",
       "5198  [0.32020592598212944, 0.0, 0.6986311112337369,...  \n",
       "5643  [0.29275678353878465, 0.0, 0.6830991615904976,...  \n",
       "\n",
       "[1300 rows x 5 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[['test_data','Sarcasm',\"cbow_vectors\",\"skip_vectors\",\"idf_vector\"]].to_excel('test_data.xlsx', index=False)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dbf709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578131ff-9afd-4a08-9c80-510b785d4cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
